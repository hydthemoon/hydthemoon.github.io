I"!<h2 id="개요">개요</h2>

<p>10장에서는 <strong>결정적 근사</strong>를 기반으로한 추론 알고리즘을 배웠다면, 이번 장에서는 <strong>수치적 표집법</strong>에 기반을 둔 근사 추론법인 Monte Carlo 테크닉에 대해 다룬다.</p>

<hr />

<p>운이 좋다면 latent variabl에 대한 사후분포를 직접적으로 구할 수 있겠지만 사실은 대부분의 경우 분포의 기댓값 정보만 필요로 한다. 따라서 이번 장에서는 기댓값만 관심을 가져보자. 연속 변수일 때는 다음과 같이 기댓값이 표현된다(이산일 때는 sum으로)</p>

<p>[E[f] = \int f(z)p(z)dz]</p>

<p>이번 단원에서는 이 적분을 구하기 복잡한 상황에서 어떻게 표집법을 활용해 기댓값을 얻어낼 것인지에 대해 다루는데, 이 idea는 분포 ${p(z)}$로 부터 독립적으로 표본 집합 ${z^{(l)}}$을 얻어내는 것에서 시작한다! 독립적으로 잘 뽑았다고 하면 다음과 같이 기댓값을 바꿔줄 수 있다. 물론 근사치이다.</p>

<p>[\hat{f} = \frac{1}{L}\sum_{l \in L}{f(z)^{(l)}}]</p>

<p>이 추정량의 분산은 공식에 의해 다음과 같이 나타낼 수 있다.</p>

<p>[var[\hat{f}] = \frac{1}{L}E[(f-E[f])^2]\qquad{(11.3)}]</p>

<p>놀라운 점은 실제로 10~20개 정도의 샘플로 기댓값을 꽤 정확하게 얻어낼 수 있다고 한다. 다만 당연하게도 이 샘플들을 <strong>독립적</strong>으로 얻어내는 것이 
중요하다.</p>

<hr />

<p>방향성 그래프(directed acyclic graph)의 경우 8장에서 다룬 ancestral sampling 기법을 단순히 적용해주면 될 것이다.</p>

<p>그런데, observed node가 존재할 경우 어떻게 할까? 생각해볼 수 있는 가장 단순한 방법은 똑같이 ancestal sampling을 사용하되, 관찰값과 다를 경우 버리고 같을 경우에만 가져가는 방식을 취해주는 방식이다. 물론 observed node들이 늘어날 수록 버리는 경우가 늘어나기 때문에, 실제로 잘 쓰이지는 않는다.</p>

<h2 id="111-기본적인-표집-알고리즘">11.1 기본적인 표집 알고리즘</h2>

<p>가장 단순한 방법으로 내장된 rand 함수를 통해 쉽게 구현할 수 있는 방법을 소개한다. 여기서 rand 함수가 독립적인 난수를 생성한다고 가정하는데, <del><a href="https://en.wikipedia.org/wiki/Random_number_generation">사실은 그렇지 않다고 알려져 있다</a>.</del></p>

<h4 id="1111-표준분포">11.1.1 표준분포</h4>

<p>단일 변수 확률 분포 ${p(z)}$가 존재할 때 어떻게 표집하면 분포를 따라갈 수 있을지 생각을 먼저해보자.</p>

<p>당연하게도 확률이 높은 부분일 수록 많이 뽑히고, 적은 부분일 수록 적게 뽑혀야할 것이다. 확률 분포(pdf)의 누적 확률 분포(cdf)를 생각해보면, 높은 확률을 가지는 구간에서는 높은 기울기를 가질 것이다. 그렇다면 여기서 어떻게 추출하면 될까? cdf는 0~1의 함숫값을 가지고 있다는 사실로부터 힌트를 얻을 수 있다. 바로 cdf의 y축(0~1)에서 random sampling을 해주면 될 것이다.즉, 역함수의 정의역에서 샘플을 취해주면 된다는 것이다. 느낌으로는 꽤나 직관적인데 이를 증명해보면 아래와 같이 간단히 나타낼 수 있다.</p>

<p>[since \quad F^{-1}(u) = inf{x \mid F(x) \geq u} \quad \forall u \in [0,1]]</p>

<p>[P(F^{-1}(U) \leq x) = P(Y \leq F(x)) = F(x)]</p>

<p>마지막 등호는 ${U}$가 uniform하기 때문에 성립한다.</p>

<p>따라서 ${F^{-1}(U)}$의 cdf는 ${F(x)}$와 같은 분포이며, continuous pdf와 cdf는 1to1 correspondence가 보장되므로 증명이 완료된다.</p>

<p>단일변수에 대해 다뤘으므로 다변수에 대해서도 다룰 수 있는데, 독립 변수의 경우에는 단일 변수일 때와 마찬가지로 각각에 대해 inverse cdf를 구해서 sampling을 진행하면 된다. 책에서는 gaussian integral을 활용한 다변수 분포의 간소화 예시인 Box-Muller Method를 소개하는데, 마치 모든 다변수에서 그러는 것처럼 설명되어 있다.</p>

<p>uniform한 ${U_1, U_2}$가 주어져있을 때, Gaussian 분포를 따르는 ${z_1, z_2}$를 어떻게 구할지 생각해보자(이부분은 굳이 안봐도 될듯).</p>

<h4 id="첫번째-방법">첫번째 방법</h4>

<p>앞서 설명한 가장 단순한 방법은 각자 inverse cdf를 구해주는 방법이다. cdf는 다음과 같다.</p>

<p>[F(z)= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{z}\exp\left{-\frac{u^2}{2}\right} du]</p>

<p>inverse를 구해주면, 아래와 같다. erfinv는 <a href="https://en.wikipedia.org/wiki/Error_function#:~:text=Inverse%20functions,-Inverse%20error%20function&amp;text=For%20real%20x%2C%20there%20is,defined%20as%20erfi%E2%88%921%20x.&amp;text=where%20ck%20is%20defined%20as%20above.">여기</a>를 참고</p>

<p>[F^{-1}(U) = \sqrt{2}: \textbf{erfinv} (2U-1)]</p>

<p>여기서 ${U_1, U_2}$를 사용해 각 ${z_1, z_2}$를 구할 수 있다.</p>

<h4 id="두번째-방법">두번째 방법</h4>

<p>erf는 anayltic하지 않은 영 좋지 않은 함수이기에 gaussian integral에서 배운 극좌표 변환 테크닉을 이용해 간단해질 수 있다. ${z}$를 2차원 평면에 두고 ${U}$들이 ${r}$과 ${\theta}$를 표현하도록 적절히 변환해주면 될 것이다. 그 과정은 아래와 같다.</p>

<p>[\begin{aligned}f(z_1, z_2) &amp;= f(z_1)f(z_2) \ 
&amp;= \frac{1}{\sqrt{2\pi}} \exp \left{-\frac{z_1^2}{2}\right} \frac{1}{\sqrt{2\pi}} \exp \left{-\frac{z_2^2}{2}\right} <br />
&amp;= \frac{1}{2 \pi} \exp \left{\frac{z_1^2 + z_2^2}{2}\right} \end{aligned}]</p>

<p>여기서 아래와 같이 둔다.</p>

<p>[z_1 = \sqrt{2} s \cos(\theta) \z_2=\sqrt{2} s\sin(\theta)]</p>

<p>그러면, 아래와 같이 단순해진다.</p>

<p>[f(z_1,z_2) = \frac{1}{2\pi}e^{-s}]</p>

<p>즉 두 pdf의 곱으로 생각할 수 있고 각각에 대해 inverse cdf를 구해주면 다음과 같다.</p>

<p>[\begin{aligned}\theta &amp;= 2\pi U_1 \ r &amp;= -\ln(1-U_2)\end{aligned}]</p>

<p>즉, 우리의 목표인 ${z_1, z_2}$는 다음과 같이 구해진다.</p>

<p>[x = \sqrt{-2\ln(U_1)} \cos (2\pi U_2) \ y = \sqrt{-2\ln(U_1)} \sin (2\pi U_2)]</p>

<p>uniform 만 잘 뽑혔다면 두 방법은 완전히 같은 결과를 보여준다.</p>

<p>이번 경우는 운이 좋은 경우였고, 그렇지 않은 경우들에 대해 일반적인 방법이 필요하다. 특별히 단변량 분포에 대해서는 rejection sampling과 importance sampling 기법이 있는데 이에 대해 알아보자.</p>

<h2 id="1112-rejection-sampling-거부-표집법">11.1.2 Rejection Sampling (거부 표집법)</h2>

<p>일반적인 분포 ${p(z)}$에서 표집을 할때 사용하는 간단한 기법이다.</p>

<ol>
  <li>${p(z)}$와 개형이 비슷한 적절하게 해석적인 함수 ${q(z)}$를 잡는다.</li>
  <li>모든 구간에서 ${kq(z) \geq p(z)}$를 만족하는 상수 ${k}$를 구한다. 작을수록 좋다!</li>
  <li>${q(z)}$를 앞서 배운 방법들로 sampling해 ${z_0}$를 얻는다.</li>
  <li>구간 ${[0, kq(z_0)]}$에서 uniform sampling을 해 ${u)0}$를 얻는다.</li>
  <li>만약 ${u_0 &gt; p(z)}$면 버리고(reject) 그렇지 않다면 유지한다.</li>
</ol>

<p><img src="http://localhost:4000/assets/img/20220705.png" alt="" /></p>

<h2 id="1113-adaptive-rejection-sampling-적응적-거부-표집법">11.1.3 Adaptive rejection sampling (적응적 거부 표집법)</h2>

<p>적절한 ${q(z)}$를 찾는것이 어려울 경우를 위한 방법이다.</p>

<p>먼저, ${\ln(p(z))}$가 오목함수인 경우에는 단순히 <a href="https://en.wikipedia.org/wiki/Linear_interpolation">line interpolation method</a>로 해결이 가능하다. numerical analysis 첫시간에 배우는 무식한 방법이지만 빠르고 좋은 방법이다. 여기서도 볼줄이야. 당연히 수집하는 point의 수가 늘어날 수록 ${p(z)}$와 ${q(z)}$가 비슷해질 것이며 rejection rate도 줄어들 것이다.</p>

<ul>
  <li>각 point에서 미분이 정의되지 않는데 변형 알고리즘이 존재한다고 한다.</li>
  <li>로그 오목이 아닐경우 11.2.2에서 살펴본다. aka Adaptive Rejection Metropolis Sampling</li>
  <li>고차원 공간에서는 acception rate가 기하급수적으로 감소하기 때문에 이를 개선하기 위해 복잡도가 급격히 증가하기에 사용하기 어렵다.</li>
</ul>

<h2 id="1114-importance-sampling-중요도-표집법">11.1.4 Importance Sampling (중요도 표집법)</h2>
:ET