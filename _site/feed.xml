<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-07-19T19:26:47+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">달숨기자 블로그</title><subtitle>hi there</subtitle><author><name>Jaehyun Kim</name></author><entry><title type="html">[JAVA] 클래스/인스턴스 변수/메서드의 구분</title><link href="http://localhost:4000/java/%EC%9E%90%EB%B0%94%EA%B3%B5%EB%B6%80/" rel="alternate" type="text/html" title="[JAVA] 클래스/인스턴스 변수/메서드의 구분" /><published>2022-07-19T00:00:00+09:00</published><updated>2022-07-19T00:00:00+09:00</updated><id>http://localhost:4000/java/%EC%9E%90%EB%B0%94%EA%B3%B5%EB%B6%80</id><content type="html" xml:base="http://localhost:4000/java/%EC%9E%90%EB%B0%94%EA%B3%B5%EB%B6%80/"><![CDATA[<p>코테 언어 3대장 중 초심자가 봤을 때 가장 어려워보이는 언어는 자바가 아닐까 싶다. C 조금 하다가 파이썬 원툴로 잘 살아왔지만 회사에서 자바를 쓰기 때문에.. 그리고 앞으로 자바민국에서 살아가기 위해 나도 자바 공부를 시작했는데, 이것도 정리하면서 공부하면 용어를 좀더 오래 기억하는데도움이 되지 않을까 싶어 정리해보기로 했다.</p>

<h2 id="필드에서의-구분">필드에서의 구분</h2>

<ul>
  <li>class variable (클래스 변수, static 변수)</li>
  <li>instance variable (인스턴스 변수)</li>
  <li>local variable (지역 변수)
static/instance 변수들의 초깃값은 적절히 설정되어 있음.</li>
</ul>

<p>앞에 static 냅다 붙어있으면 class variable 되는거다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"><center>타입</center></th>
      <th style="text-align: center"><center>생성시기</center></th>
      <th style="text-align: right"><center>소멸시기</center></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>class</strong></td>
      <td style="text-align: center"><center>클래스가 메모리에 로드될 때</center></td>
      <td style="text-align: right"><center>프로그램이 종료될 때 </center></td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>instance</strong></td>
      <td style="text-align: center"><center>인스턴스가 생성될 때</center></td>
      <td style="text-align: right"><center> 인스턴스가 소멸할때/GC가 수거할 때 </center></td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>local</strong></td>
      <td style="text-align: center"><center>블록 내의 변수 선언문이 실행될 때</center></td>
      <td style="text-align: right"><center> 블록을 벗어날 때 </center></td>
    </tr>
  </tbody>
</table>

<p>class 변수는 instance를 생성하기 전부터 이미 메모리(method영역)에 로드되어 있다. 때문에, 해당 클래스의 모든 instance는 같은 값을 가지게 된다. 따라서, shared variable(공유 변수)라고도 불림. 사실 이 문단만 기억하면 될듯.</p>

<p>instance 변수는 heap영역에 저장되어 instance마다 개별 지정 가능.</p>

<p>local 변수는 stack에 저장됨.</p>

<h3 id="예시">예시</h3>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">Car</span> <span class="o">{</span>
  <span class="kt">int</span> <span class="n">limit</span> <span class="o">=</span> <span class="mi">500</span><span class="o">;</span> <span class="c1">// instance variable</span>
  <span class="kd">static</span> <span class="nc">String</span> <span class="n">name</span> <span class="o">=</span> <span class="err">'</span><span class="n">ionic</span><span class="err">'</span><span class="o">;</span> <span class="c1">// static variable</span>
  <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span> <span class="n">args</span><span class="o">[]){</span>
    <span class="kt">int</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">1</span><span class="o">;</span> <span class="c1">// local variable</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<h2 id="메소드에서의-구분">메소드에서의 구분</h2>

<ul>
  <li>class method (클래스 메소드, static method)</li>
  <li>instance method (인스턴스 메소드)</li>
</ul>

<p>비슷하게 static 키워드가 달려있으면 클래스되는거고 없으면 인스턴스되는 것이다. 또 비슷하게 class method는 instance 생성 전부터 사용가능하다.</p>

<p>따라서, class method는 instance variable을 사용할 수 없는데, 생각해보면 class들은 바로 생기지만 instance들은 instance가 할당되어야 시작되므로 순서가 꼬이지 않기 위해서 그렇다고 보면 된다.</p>

<p>따라서, method가 instance method/variable을 사용하지 않으면 보통 class method로 해준다고 한다.</p>

<h3 id="예시-1">예시</h3>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">Car</span> <span class="o">{</span>
  <span class="kt">boolean</span> <span class="n">door</span><span class="o">;</span>
  <span class="kt">void</span> <span class="nf">openDoor</span><span class="o">(){</span> <span class="c1">// instance method</span>
    <span class="n">door</span> <span class="o">=</span> <span class="kc">true</span><span class="o">;</span>
  <span class="o">}</span>
  <span class="kd">static</span> <span class="kt">void</span> <span class="nf">toggleDoor</span><span class="o">(</span><span class="kt">boolean</span> <span class="n">d</span><span class="o">)</span> <span class="o">{</span> <span class="c1">// class method</span>
    <span class="k">return</span> <span class="o">!</span><span class="n">d</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<hr />

<p>각 변수가 어떻게 저장되는지 더 찾아보다가 JAVA에서 GC가 메모리를 어떻게 관리하는지에 대해 공부하게 되었는데 다음 포스트에서 다룰 예정이다.</p>]]></content><author><name>Jaehyun Kim</name></author><category term="JAVA" /><category term="JAVA" /><category term="cs" /><summary type="html"><![CDATA[코테 언어 3대장 중 초심자가 봤을 때 가장 어려워보이는 언어는 자바가 아닐까 싶다. C 조금 하다가 파이썬 원툴로 잘 살아왔지만 회사에서 자바를 쓰기 때문에.. 그리고 앞으로 자바민국에서 살아가기 위해 나도 자바 공부를 시작했는데, 이것도 정리하면서 공부하면 용어를 좀더 오래 기억하는데도움이 되지 않을까 싶어 정리해보기로 했다. 필드에서의 구분 class variable (클래스 변수, static 변수) instance variable (인스턴스 변수) local variable (지역 변수) static/instance 변수들의 초깃값은 적절히 설정되어 있음. 앞에 static 냅다 붙어있으면 class variable 되는거다. 타입 생성시기 소멸시기 class 클래스가 메모리에 로드될 때 프로그램이 종료될 때 instance 인스턴스가 생성될 때 인스턴스가 소멸할때/GC가 수거할 때 local 블록 내의 변수 선언문이 실행될 때 블록을 벗어날 때 class 변수는 instance를 생성하기 전부터 이미 메모리(method영역)에 로드되어 있다. 때문에, 해당 클래스의 모든 instance는 같은 값을 가지게 된다. 따라서, shared variable(공유 변수)라고도 불림. 사실 이 문단만 기억하면 될듯. instance 변수는 heap영역에 저장되어 instance마다 개별 지정 가능. local 변수는 stack에 저장됨. 예시 class Car { int limit = 500; // instance variable static String name = 'ionic'; // static variable public static void main(String args[]){ int a = 1; // local variable } } 메소드에서의 구분 class method (클래스 메소드, static method) instance method (인스턴스 메소드) 비슷하게 static 키워드가 달려있으면 클래스되는거고 없으면 인스턴스되는 것이다. 또 비슷하게 class method는 instance 생성 전부터 사용가능하다. 따라서, class method는 instance variable을 사용할 수 없는데, 생각해보면 class들은 바로 생기지만 instance들은 instance가 할당되어야 시작되므로 순서가 꼬이지 않기 위해서 그렇다고 보면 된다. 따라서, method가 instance method/variable을 사용하지 않으면 보통 class method로 해준다고 한다. 예시 class Car { boolean door; void openDoor(){ // instance method door = true; } static void toggleDoor(boolean d) { // class method return !d; } } 각 변수가 어떻게 저장되는지 더 찾아보다가 JAVA에서 GC가 메모리를 어떻게 관리하는지에 대해 공부하게 되었는데 다음 포스트에서 다룰 예정이다.]]></summary></entry><entry><title type="html">[PRML] 11장 표집법</title><link href="http://localhost:4000/ml/PRML-%ED%91%9C%EC%A7%91%EB%B2%95/" rel="alternate" type="text/html" title="[PRML] 11장 표집법" /><published>2022-07-15T00:00:00+09:00</published><updated>2022-07-15T00:00:00+09:00</updated><id>http://localhost:4000/ml/%5BPRML%5D-%ED%91%9C%EC%A7%91%EB%B2%95</id><content type="html" xml:base="http://localhost:4000/ml/PRML-%ED%91%9C%EC%A7%91%EB%B2%95/"><![CDATA[<h2 id="개요">개요</h2>

<p>10장에서는 <strong>결정적 근사</strong>를 기반으로한 추론 알고리즘을 배웠다면, 이번 장에서는 <strong>수치적 표집법</strong>에 기반을 둔 근사 추론법인 Monte Carlo 테크닉에 대해 다룬다.</p>

<hr />

<p>운이 좋다면 latent variable에 대한 사후분포를 직접적으로 구할 수 있겠지만 사실은 대부분의 경우 분포의 기댓값 정보만 필요로 한다. 따라서 이번 장에서는 기댓값만 관심을 가져보자. 연속 변수일 때는 다음과 같이 기댓값이 표현된다(이산일 때는 sum으로)</p>

\[E[f] = \int f(z)p(z)dz\]

<p>이번 단원에서는 이 적분을 구하기 복잡한 상황에서 어떻게 표집법을 활용해 기댓값을 얻어낼 것인지에 대해 다루는데, 이 idea는 분포 ${p(z)}$로 부터 독립적으로 표본 집합 ${z^{(l)}}$을 얻어내는 것에서 시작한다! 독립적으로 잘 뽑았다고 하면 다음과 같이 기댓값을 바꿔줄 수 있다. 물론 근사치이다.</p>

\[\hat{f} = \frac{1}{L}\sum_{l \in L}{f(z)^{(l)}}\]

<p>이 추정량의 분산은 공식에 의해 다음과 같이 나타낼 수 있다.</p>

\[var[\hat{f}] = \frac{1}{L}E[(f-E[f])^2]\qquad{(11.3)}\]

<p>놀라운 점은 실제로 10~20개 정도의 샘플로 기댓값을 꽤 정확하게 얻어낼 수 있다고 한다. 다만 당연하게도 이 샘플들을 <strong>독립적</strong>으로 얻어내는 것이 
중요하다.</p>

<hr />

<p>방향성 그래프(directed acyclic graph)의 경우 8장에서 다룬 ancestral sampling 기법을 단순히 적용해주면 될 것이다.</p>

<p>그런데, observed node가 존재할 경우 어떻게 할까? 생각해볼 수 있는 가장 단순한 방법은 똑같이 ancestal sampling을 사용하되, 관찰값과 다를 경우 버리고 같을 경우에만 가져가는 방식을 취해주는 방식이다. 물론 observed node들이 늘어날 수록 버리는 경우가 늘어나기 때문에, 실제로 잘 쓰이지는 않는다.</p>

<h2 id="111-기본적인-표집-알고리즘">11.1 기본적인 표집 알고리즘</h2>

<p>가장 단순한 방법으로 내장된 rand 함수를 통해 쉽게 구현할 수 있는 방법을 소개한다. 여기서 rand 함수가 독립적인 난수를 생성한다고 가정하자. <del><a href="https://en.wikipedia.org/wiki/Random_number_generation">사실은 그렇지 않다고 알려져 있다</a>.</del></p>

<h3 id="1111-표준분포">11.1.1 표준분포</h3>

<p>단일 변수 확률 분포 ${p(z)}$가 존재할 때 어떻게 표집하면 분포를 따라갈 수 있을지 생각을 먼저해보자.</p>

<p>당연하게도 확률이 높은 부분일 수록 많이 뽑히고, 적은 부분일 수록 적게 뽑혀야할 것이다. 확률 분포(pdf)의 누적 확률 분포(cdf)를 생각해보면, 높은 확률을 가지는 구간에서는 높은 기울기를 가질 것이다. 그렇다면 여기서 어떻게 추출하면 될까? cdf는 0~1의 함숫값을 가지고 있다는 사실로부터 힌트를 얻을 수 있다. 바로 cdf의 y축(0~1)에서 random sampling을 해주면 될 것이다.즉, 역함수의 정의역에서 샘플을 취해주면 된다는 것이다. 느낌으로는 꽤나 직관적인데 이를 증명해보면 아래와 같이 간단히 나타낼 수 있다.</p>

\[since \quad F^{-1}(u) = inf\{x \mid F(x) \geq u\} \quad \forall u \in [0,1]\]

\[P(F^{-1}(U) \leq x) = P(Y \leq F(x)) = F(x)\]

<p>마지막 등호는 ${U}$가 uniform하기 때문에 성립한다.</p>

<p>따라서 ${F^{-1}(U)}$의 cdf는 ${F(x)}$와 같은 분포이며, continuous pdf와 cdf는 1to1 correspondence가 보장되므로 증명이 완료된다.</p>

<p>단일변수에 대해 다뤘으므로 다변수에 대해서도 다룰 수 있는데, 독립 변수의 경우에는 단일 변수일 때와 마찬가지로 각각에 대해 inverse cdf를 구해서 sampling을 진행하면 된다. 책에서는 gaussian integral을 활용한 다변수 분포의 간소화 예시인 Box-Muller Method를 소개하는데, 마치 모든 다변수에서 그러는 것처럼 설명되어 있다.</p>

<p>uniform한 ${U_1, U_2}$가 주어져있을 때, Gaussian 분포를 따르는 ${z_1, z_2}$를 어떻게 구할지 생각해보자(이부분은 굳이 안봐도 될듯).</p>

<p><strong>첫번째 방법</strong></p>

<p>앞서 설명한 가장 단순한 방법은 각자 inverse cdf를 구해주는 방법이다. cdf는 다음과 같다.</p>

\[F(z)= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{z}\exp\left\{-\frac{u^2}{2}\right\} du\]

<p>inverse를 구해주면, 아래와 같다. erfinv는 <a href="https://en.wikipedia.org/wiki/Error_function#:~:text=Inverse%20functions,-Inverse%20error%20function&amp;text=For%20real%20x%2C%20there%20is,defined%20as%20erfi%E2%88%921%20x.&amp;text=where%20ck%20is%20defined%20as%20above.">여기</a>를 참고</p>

\[F^{-1}(U) = \sqrt{2}\: \textbf{erfinv} (2U-1)\]

<p>여기서 ${U_1, U_2}$를 사용해 각 ${z_1, z_2}$를 구할 수 있다.</p>

<p><strong>두번째 방법</strong></p>

<p>erf는 anayltic하지 않은 영 좋지 않은 함수이기에 gaussian integral에서 배운 극좌표 변환 테크닉을 이용해 간단해질 수 있다. ${z}$를 2차원 평면에 두고 ${U}$들이 ${r}$과 ${\theta}$를 표현하도록 적절히 변환해주면 될 것이다. 그 과정은 아래와 같다.</p>

\[\begin{aligned}f(z_1, z_2) &amp;= f(z_1)f(z_2) \\ 
&amp;= \frac{1}{\sqrt{2\pi}} \exp \left\{-\frac{z_1^2}{2}\right\} \frac{1}{\sqrt{2\pi}} \exp \left\{-\frac{z_2^2}{2}\right\} \\
&amp;= \frac{1}{2 \pi} \exp \left\{\frac{z_1^2 + z_2^2}{2}\right\} \end{aligned}\]

<p>여기서 아래와 같이 둔다.</p>

\[z_1 = \sqrt{2} s \cos(\theta) \\z_2=\sqrt{2} s\sin(\theta)\]

<p>그러면, 아래와 같이 단순해진다.</p>

\[f(z_1,z_2) = \frac{1}{2\pi}e^{-s}\]

<p>즉 두 pdf의 곱으로 생각할 수 있고 각각에 대해 inverse cdf를 구해주면 다음과 같다.</p>

\[\begin{aligned}\theta &amp;= 2\pi U_1 \\ r &amp;= -\ln(1-U_2)\end{aligned}\]

<p>즉, 우리의 목표인 ${z_1, z_2}$는 다음과 같이 구해진다.</p>

\[z_1 = \sqrt{-2\ln(U_1)} \cos (2\pi U_2) \\ z_2 = \sqrt{-2\ln(U_1)} \sin (2\pi U_2)\]

<p>uniform 만 잘 뽑혔다면 두 방법은 완전히 같은 결과를 보여준다.</p>

<p>이번 경우는 운이 좋은 경우였고, 그렇지 않은 경우들에 대해 일반적인 방법이 필요하다. 특별히 단변량 분포에 대해서는 rejection sampling과 importance sampling 기법이 있는데 이에 대해 알아보자.</p>

<h3 id="1112-거부-표집법-rejection-sampling">11.1.2 거부 표집법 (Rejection Sampling)</h3>

<p>일반적인 분포 ${p(z)}$에서 표집을 할때 사용하는 간단한 기법이다.</p>

<ol>
  <li>${p(z)}$와 개형이 비슷한 적절하게 해석적인 함수 ${q(z)}$를 잡는다.</li>
  <li>모든 구간에서 ${kq(z) \geq p(z)}$를 만족하는 상수 ${k}$를 구한다. 작을수록 좋다!</li>
  <li>${q(z)}$를 앞서 배운 방법들로 sampling해 ${z_0}$를 얻는다.</li>
  <li>구간 ${[0, kq(z_0)]}$에서 uniform sampling을 해 ${u)0}$를 얻는다.</li>
  <li>만약 ${u_0 &gt; p(z)}$면 버리고(reject) 그렇지 않다면 유지한다.</li>
</ol>

<h3 id="1113-적응적-거부-표집법">11.1.3 적응적 거부 표집법</h3>

<p>적절한 ${q(z)}$를 찾는것이 어려울 경우를 위한 방법(야매)이다.</p>

<p>먼저, ${\ln(p(z))}$가 오목함수인 경우에는 단순히 <a href="https://en.wikipedia.org/wiki/Linear_interpolation">line interpolation method</a>로 해결이 가능하다. numerical analysis 첫시간에 배우는 무식한 방법이지만 빠르고 좋은 방법이다. 여기서도 볼줄이야. 당연히 수집하는 point의 수가 늘어날 수록 ${p(z)}$와 ${q(z)}$가 비슷해질 것이며 rejection rate도 줄어들 것이다.</p>

<ul>
  <li>각 point에서 미분이 정의되지 않는데 변형 알고리즘이 존재한다고 한다.</li>
  <li>로그 오목이 아닐경우 11.2.2에서 살펴본다. aka Adaptive Rejection Metropolis Sampling</li>
  <li>고차원 공간에서는 acception rate가 기하급수적으로 감소하기 때문에 이를 개선하기 위해 복잡도가 급격히 증가하기에 사용하기 어렵다.</li>
</ul>

<h3 id="1114-중요도-표집법">11.1.4 중요도 표집법</h3>

<p>역시 표집하기 어려운 분포가 주어졌다고 가정하고 기닷갮 ${E(z)}$를 구해보자.</p>

<p>이번에도 새로운 분포 ${q(z)}$를 두고 일단 샘플링을 해서 ${z^l}$들을 얻어내고 해당 ${z^l}$에서 ${\frac{p(z)}{q(z)}}$만큼의 weight를 주고 기댓값을 계산해주는 방식이다. 기본 idea는 Rejection Sampling과 다를바 없어 보인다.</p>

<p>논리대로 식을 전개해보자면 다음과 같다.</p>

\[E[f] = \int f(z)p(z)dz = \frac{Z_q}{Z_p}\int f(z)\frac{\tilde{p}(z)}{\tilde{q}(z)}q(z)dz \simeq \frac{Z_q}{Z_p}\frac{1}{L} \sum_{l=1}^{L} \tilde{r}_{l} f(z^{(l)})\]

<p>마찬가지로 ${q(z)}$가 얼마나 ${p(z)}$와 비슷한 분포를 가지는지, ${p(z)}$의 자연스러운 분포 여부(적당히 생긴)에 따라 기댓값의 정확도가 상승한다.</p>

<ul>
  <li>그래프 모델에서는 여기에 마르코프한 가중치를 덮어주면 된다. 또한, 이를 발전시킨 형태로 Likelihood Weighted Sampling(가능도 가중 표집법)이 존재한다. 이는 Ancestral Sampling(조상 표집법)을 적용한 형태로 진행. 더 나아가 Self Importance Sampling(자가 중요도 표집법)도 있는데 그만 알아보자.</li>
</ul>

<h3 id="1115-표집-중요도-재표집">11.1.5 표집 중요도 재표집</h3>

<p>Rejection Sampling을 할때 ${k}$를 구하는 과정이 있었는데, 이 과정을 생략해주는 방법이다.</p>

<ol>
  <li>${q(z)}$로부터 ${z^l}$을 추출한다.</li>
  <li>각 ${z^l}$에 대한 가중치 ${w_l}$을 계산한다.</li>
  <li>각각의 확률이 ${w_l}$을 가지는 이산 분포 ${z^l}$들로부터 다시 ${L}$개를 추출한다.</li>
</ol>

<p>결국 Rejection Sampling에 Importance Sampling을 덮어준 형태인데, 기본 idea는 같은 것이라 크게 발전된 형태로 보이지는 않는다.</p>

<h3 id="1116-표집법과-em-알고리즘">11.1.6 표집법과 EM 알고리즘</h3>

<p>EM을 사용할 때, E단계에서 non-analytic한 함수를 만났을 경우 표집법을 통한 근사로 해결이 가능하다.</p>

<p>즉, 적분꼴로 표현된 최적화 대상을 이산 합 형태로 아래와 같이 바꿔주는 것이다.</p>

\[Q(\theta, \theta^{old}) = \int p(\textbf{Z} | \textbf{X} , \theta^{old}) \ln p(\textbf{Z},\textbf{X} | \theta) \, d\textbf{Z}\]

\[to\]

\[Q(\theta, \theta^{old}) \approx \frac{1}{L} \sum \ln p(\textbf{Z}^{(l)}, \textbf{X} \bar \theta )\]

<p>이 과정을 Monte Carlo EM algorithm이라고 부른다.</p>

<p>참고) IP 알고리즘</p>

<p>${p(\textbf{Z} \mid \textbf{X})}$에서 표집이 어려운 상황에서 Maximum Likelihood 대신 Bayesian하게 접근해서 해결하는 방법.</p>

<ul>
  <li>I Step</li>
</ul>

\[p(\textbf{Z} | \textbf{X}) = \int p(\textbf{Z} | \textbf{X} , \theta) p(\theta | \textbf{X}) \, d\theta\]

<p>순서대로 ${\textbf{X}}$로부터 ${\theta ^{(l)}}$들을 추출하고 이들의 결합분포를 통해 ${p(\textbf{Z} \mid \textbf{X} , \theta)}$에서 ${\textbf{X}}$를 추출한다.</p>

<ul>
  <li>P Step</li>
</ul>

\[p(\theta | \textbf{X}) = \int p(\theta | \textbf{Z} , \textbf{X}) p(\textbf{Z} | \textbf{X}) \, d\textbf{Z}\]

<p>구한 ${\textbf{Z}}$들을 통해 ${\theta}$에 대한 사후분포식을 수정해준다.</p>

<h2 id="112-마르코프-연쇄-몬테-카를로">11.2 마르코프 연쇄 몬테 카를로</h2>

<p>MCMC는 Rejection Sampling이나 Importance Sampling과 비슷한 방식으로 진행되지만, 무조건 Reject되기 보다는 회생의 기회를 준다는점에서 차이가 있다. 물론 여기서 얻은 표본들도 독립이 아니지만, 충분히 큰 M에 대해 kM번째 샘플만 취해준다면 어느정도 독립성을 보장해줄 수 있다.</p>

<p>먼저, 가장 대표적인 방식인 Metropolis Algorithm에 대해 알아보자.</p>

<ol>
  <li>목표 분포 ${p(z)}$에 대해 Rejection Sampling때와 마찬가지로 적절한 ${q(z)}$를 잡아준다.
(Metropolis 알고리즘에서는 대칭(${q(z_A \mid z_B) = q(z_B \mid z_A)}$)임을 가정한다.)</li>
  <li>초깃값 ${z^{(1)}}$을 시작으로 ${q(z)}$ 에서 하나의 표본 ${z^*}$을 추출한다.</li>
  <li>Accept or Reject
    <ol>
      <li>${p(z^*) &gt; p(z^{(1)})}$일경우 accept</li>
      <li>그렇지 않을 경우, $\frac{p(z^*)}{p(z^{(1)})}$의 확률로 accept한다.</li>
      <li>이를 다시표현하면, ${min \left(1, \frac{p(z^*)}{p(z^{(1)})}\right)}$의 확률로 Accept한다.</li>
    </ol>
  </li>
</ol>

<p>이에 대한 증명은 적혀있지 않은데, <a href="https://hydthemoon.github.io/ml/PRML-%ED%91%9C%EC%A7%91%EB%B2%95/#1122-metropolis-hastings-algorithm">11.2.2</a>에서 일반화된 증명으로 살펴본다.</p>

<h3 id="1121-마르코프-연쇄">11.2.1 마르코프 연쇄</h3>

<p>다음 상태가 현재 상태에만 의존할 경우 마르코프하다고 한다.</p>

\[{ p(z^{(m+1)} \mid z^{(1)}, \ldots z^{(m)}) = p(z^{(m+1)}\mid z^{(m)} )}\]

<p>각 상태에서 ${T_{m}^{ij} = p(z^{(m+1)} = v_j \mid z^{(m)} = v_i) }$을 Transition Matrix라고 부르며 이 matrix가 ${m}$에 무관할 경우 Homogenous한 chain이라고 부른다.</p>

<p>최종적으로 ergodic 개념이 중요한데, 아래 세 조건은 동치이다.</p>

<ol>
  <li>Ergodic</li>
  <li>Positive Recurrent, Aperiodic</li>
  <li>Finite, Irreducible, Aperiodic</li>
</ol>

<p>즉, ergodic하면, 충분한 시간이 흐른 뒤 분포와 초기 분포가 동일한 분포가 된다.</p>

<p>또한, 헷갈리는 개념으로 stationary가 있는데, transition matrix ${T}$에 대해 ${\pi = \pi T}$를 만족할 때, 즉, 분포가 일정할 때 stationary(invariant)하다고 한다.</p>

\[p(z^{(m+1)}) = \sum p(z^{(m+1)} \mid z^{(m)}) p(z^{(m)})\]

<p>일반적으로 ergodic하면 stationary한 ${\pi}$가 존재하며 stationary하다.</p>

<p>자세한 내용이 궁금하다면 Stochastic Process를 공부해보자. 재밌는 책이다.</p>

<h3 id="1122-메트로폴리스-헤이스팅스-알고리즘">11.2.2 메트로폴리스 헤이스팅스 알고리즘</h3>

<p>앞선 Metropolis Algorithm은 대칭 ${q(z)}$만을 사용했는데, 한계가 있으므로 이번에는 좀더 일반적인 상황을 위한 해법을 살펴보자.</p>

<p>유일한 차이는 Accept or Reject할 때이다. 다음과 같이 acceptance rate를 정규화해주면 된다.</p>

\[rate_{z, z'} = min\left(1, \frac{\frac{p(z^*)}{q(z^* \mid z^m)}}{\frac{p(z^m)}{q(z^m \mid z^*)}} \right)\]

<p>이제 이렇게 얻은 표본집합이 ${p(z)}$에 수렴한다는 것을 증명해야한다. 책에는 좀 대충나와있는 것 같다.</p>

<p>일단, 각 표본들은 위의 rate를 따르는 MC라고 볼 수 있다. 나아가, acceptance rate를 통해 postivie recurrent함을 확인할 수 있고, 당연하게 Aperiodic하므로, Ergodic한 MC임을 쉽게 볼 수 있다. 따라서, unique한 stationary matrix ${\pi}$를 가짐을 알 수 있다. 이제, ${\pi}$가 ${p(x)}$에 수렴함을 보여야한다. 다음 식을 살펴보자.</p>

\[\begin{aligned}p(z)q(z'\mid z) * rate_{z, z'} &amp;= min\left(p(z)q(z'\mid z), p(z')q(z\mid z')\right) \\ &amp;= min\left(p(z')q(z\mid z'), p(z)q(z'\mid z)\right)  \\ &amp;= p(z')q(z\mid z') * rate_{z', z}\end{aligned}\]

<p>따라서, ${q(z)}$와 rate의 결합은 이 MC의 transition probability라고 볼 수 있으며, 결국 같은 ${\pi}$에 수렴함을 알 수 있다.</p>

<p>마찬가지로, 어떤 ${q(z)}$를 사용하는지에 따라 알고리즘의 성능에 현저한 영향을 줄 수 있다. 일반적으로 gaussian을 사용한다. 여기서 분산을 어떻게 두냐에 따른 trade-off가 있는데, 분산이 작을 경우 acceptance는 증가하지만, 전체를 탐색하기 오래걸리고, 분산이 클 경우 acceptance 자체가 낮아지게 된다. 따라서 그림 11.10과 같이 방향에 따라 분산의 차이가 클 경우 알고리즘이 느리게 수렴하게 된다.</p>

<h2 id="113-깁스-표집법">11.3 깁스 표집법</h2>

<p>알고보니 <a href="https://en.wikipedia.org/wiki/Gibbs_free_energy">Gibbs Free Energy</a>를 정의한 그 Gibbs이다. 이쪽업계 코시 같은 사람인듯;</p>

<p>아무튼 Gibbs Sampling은 MCMC의 한 종류이다. 방법은 간단한데, ${m}$개의 확률변수들이 결합된 결합확률분포가 있을 때, ${m - 1}$개의 변수를 고정시키고 남은 한개를 조건부 확률분포로 부터 표집하고, 다른 ${m-1}$개를 고정시키고 남은 한개를 표집하고 … 을 반복해서 얻어나가는 방법이다. 너무 rough하게 적은것 같으니 좀더 정확하게 설명하면 ${T}$회 반복하는 경우 아래와 같다.</p>

<ol>
  <li>initialize ${z_i : i = 1, \ldots M}$</li>
  <li>for ${i}$ in range (${1}$, ${T}$)
    <ol>
      <li>${p(z_1 \mid z_2^{i},z_3^{i},\ldots,z_M^{i})}$에서 ${z_1^{i+1}}$을 표집한다.</li>
      <li>${p(z_2 \mid z_1^{i+1},z_3^{i},\ldots,z_M^{i})}$에서 ${z_2^{i+1}}$을 표집한다.
${\vdots}$</li>
      <li>${p(z_M \mid z_1^{i+1},z_2^{i+1},\ldots,z_{M-1}^{i+1})}$에서 ${z_M^{i+1}}$을 표집한다.</li>
    </ol>
  </li>
</ol>

<p>여기서는 변수들에게 order를 부여하고 진행했는데, random한 order로 진행할 수도 있다. 물론 결과적으로는 같다.</p>

<p>이제, 이 기법이 올바른 샘플링임을 증명해보자. MH 알고리즘의 일종이라고 언급했으니 이게 왜 MH 알고리즘인지 증명해보면 된다. 좀 rough하게 살펴보자.</p>

<p>일단, 이 분포 또한 MH에서 사용한 분포와 같이 stationary함을 보여야한다. 이는 꽤나 자명한데, 각 표집에서 적절한 조건부 분포를 사용하기 때문이다. 즉, 적할한 ${\pi}$가 존재한다.</p>

\[\begin{aligned} rate &amp;= min\left( 1, \frac{p(z^*)q(z|z^*)}{p(z)q(z^*|z) } \right)\\ &amp;= min\left(1, \frac{p(z^*_k|z^*_{-k})p(z^*_{-k})p(z_k | z^*_{-k})}{p(z_k|z_{-k})p(z_{-k})p(z^*_{k}|z_{-k})} \right) \\ &amp;= min(1,1) = 1\end{aligned}\]

<p>즉, MH와 같은 rate를 가지고 있기 때문에, MH의 일종으로 볼 수 있다.</p>

<h2 id="114-조각-표집법">11.4 조각 표집법</h2>

<p>계속해서 보이는 알고리즘들의 단점은 ${p(z)}$와 ${q(z)}$의 차이에서 비롯되었다. 분산이 작으면 탐색이 느리고, 크면 acceptance가 낮아진다. 조각표집법(slice sampling)은 새로운 방식으로 해결하고자 했다.</p>

<p>아이디어는 간단한데, 아래와 같이 y축에서 쳐다보는 변수 ${u}$를 추가해서 해결하는 것이다.</p>

\[\hat p(z,u) =  \begin{cases} 1/Z_p &amp; 0 \leq u \leq \hat p(z) \\ 0 &amp; else \end{cases}\]

<p>그리고, 깁스 샘플링 같은 방법을 통해 ${\hat p(z,u)}$에서 표본을 얻고, ${u}$를 무시해주어 원 분포를 얻는 방식이다.</p>

<p>근데 이건 그냥 원래 있던 inverse cdf하는 방법보다도 원시적인 방법이 아닌가 싶다.</p>

<h2 id="115-하이브리드-몬테-카를로-알고리즘">11.5 하이브리드 몬테 카를로 알고리즘</h2>

<h2 id="116-분할-함수-추정">11.6 분할 함수 추정</h2>]]></content><author><name>Jaehyun Kim</name></author><category term="ML" /><category term="PRML" /><category term="표집법" /><category term="MCMC" /><category term="깁스 샘플링" /><summary type="html"><![CDATA[개요 10장에서는 결정적 근사를 기반으로한 추론 알고리즘을 배웠다면, 이번 장에서는 수치적 표집법에 기반을 둔 근사 추론법인 Monte Carlo 테크닉에 대해 다룬다. 운이 좋다면 latent variable에 대한 사후분포를 직접적으로 구할 수 있겠지만 사실은 대부분의 경우 분포의 기댓값 정보만 필요로 한다. 따라서 이번 장에서는 기댓값만 관심을 가져보자. 연속 변수일 때는 다음과 같이 기댓값이 표현된다(이산일 때는 sum으로) \[E[f] = \int f(z)p(z)dz\] 이번 단원에서는 이 적분을 구하기 복잡한 상황에서 어떻게 표집법을 활용해 기댓값을 얻어낼 것인지에 대해 다루는데, 이 idea는 분포 ${p(z)}$로 부터 독립적으로 표본 집합 ${z^{(l)}}$을 얻어내는 것에서 시작한다! 독립적으로 잘 뽑았다고 하면 다음과 같이 기댓값을 바꿔줄 수 있다. 물론 근사치이다. \[\hat{f} = \frac{1}{L}\sum_{l \in L}{f(z)^{(l)}}\] 이 추정량의 분산은 공식에 의해 다음과 같이 나타낼 수 있다. \[var[\hat{f}] = \frac{1}{L}E[(f-E[f])^2]\qquad{(11.3)}\] 놀라운 점은 실제로 10~20개 정도의 샘플로 기댓값을 꽤 정확하게 얻어낼 수 있다고 한다. 다만 당연하게도 이 샘플들을 독립적으로 얻어내는 것이 중요하다. 방향성 그래프(directed acyclic graph)의 경우 8장에서 다룬 ancestral sampling 기법을 단순히 적용해주면 될 것이다. 그런데, observed node가 존재할 경우 어떻게 할까? 생각해볼 수 있는 가장 단순한 방법은 똑같이 ancestal sampling을 사용하되, 관찰값과 다를 경우 버리고 같을 경우에만 가져가는 방식을 취해주는 방식이다. 물론 observed node들이 늘어날 수록 버리는 경우가 늘어나기 때문에, 실제로 잘 쓰이지는 않는다. 11.1 기본적인 표집 알고리즘 가장 단순한 방법으로 내장된 rand 함수를 통해 쉽게 구현할 수 있는 방법을 소개한다. 여기서 rand 함수가 독립적인 난수를 생성한다고 가정하자. 사실은 그렇지 않다고 알려져 있다. 11.1.1 표준분포 단일 변수 확률 분포 ${p(z)}$가 존재할 때 어떻게 표집하면 분포를 따라갈 수 있을지 생각을 먼저해보자. 당연하게도 확률이 높은 부분일 수록 많이 뽑히고, 적은 부분일 수록 적게 뽑혀야할 것이다. 확률 분포(pdf)의 누적 확률 분포(cdf)를 생각해보면, 높은 확률을 가지는 구간에서는 높은 기울기를 가질 것이다. 그렇다면 여기서 어떻게 추출하면 될까? cdf는 0~1의 함숫값을 가지고 있다는 사실로부터 힌트를 얻을 수 있다. 바로 cdf의 y축(0~1)에서 random sampling을 해주면 될 것이다.즉, 역함수의 정의역에서 샘플을 취해주면 된다는 것이다. 느낌으로는 꽤나 직관적인데 이를 증명해보면 아래와 같이 간단히 나타낼 수 있다. \[since \quad F^{-1}(u) = inf\{x \mid F(x) \geq u\} \quad \forall u \in [0,1]\] \[P(F^{-1}(U) \leq x) = P(Y \leq F(x)) = F(x)\] 마지막 등호는 ${U}$가 uniform하기 때문에 성립한다. 따라서 ${F^{-1}(U)}$의 cdf는 ${F(x)}$와 같은 분포이며, continuous pdf와 cdf는 1to1 correspondence가 보장되므로 증명이 완료된다. 단일변수에 대해 다뤘으므로 다변수에 대해서도 다룰 수 있는데, 독립 변수의 경우에는 단일 변수일 때와 마찬가지로 각각에 대해 inverse cdf를 구해서 sampling을 진행하면 된다. 책에서는 gaussian integral을 활용한 다변수 분포의 간소화 예시인 Box-Muller Method를 소개하는데, 마치 모든 다변수에서 그러는 것처럼 설명되어 있다. uniform한 ${U_1, U_2}$가 주어져있을 때, Gaussian 분포를 따르는 ${z_1, z_2}$를 어떻게 구할지 생각해보자(이부분은 굳이 안봐도 될듯). 첫번째 방법 앞서 설명한 가장 단순한 방법은 각자 inverse cdf를 구해주는 방법이다. cdf는 다음과 같다. \[F(z)= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{z}\exp\left\{-\frac{u^2}{2}\right\} du\] inverse를 구해주면, 아래와 같다. erfinv는 여기를 참고 \[F^{-1}(U) = \sqrt{2}\: \textbf{erfinv} (2U-1)\] 여기서 ${U_1, U_2}$를 사용해 각 ${z_1, z_2}$를 구할 수 있다. 두번째 방법 erf는 anayltic하지 않은 영 좋지 않은 함수이기에 gaussian integral에서 배운 극좌표 변환 테크닉을 이용해 간단해질 수 있다. ${z}$를 2차원 평면에 두고 ${U}$들이 ${r}$과 ${\theta}$를 표현하도록 적절히 변환해주면 될 것이다. 그 과정은 아래와 같다. \[\begin{aligned}f(z_1, z_2) &amp;= f(z_1)f(z_2) \\ &amp;= \frac{1}{\sqrt{2\pi}} \exp \left\{-\frac{z_1^2}{2}\right\} \frac{1}{\sqrt{2\pi}} \exp \left\{-\frac{z_2^2}{2}\right\} \\ &amp;= \frac{1}{2 \pi} \exp \left\{\frac{z_1^2 + z_2^2}{2}\right\} \end{aligned}\] 여기서 아래와 같이 둔다. \[z_1 = \sqrt{2} s \cos(\theta) \\z_2=\sqrt{2} s\sin(\theta)\] 그러면, 아래와 같이 단순해진다. \[f(z_1,z_2) = \frac{1}{2\pi}e^{-s}\] 즉 두 pdf의 곱으로 생각할 수 있고 각각에 대해 inverse cdf를 구해주면 다음과 같다. \[\begin{aligned}\theta &amp;= 2\pi U_1 \\ r &amp;= -\ln(1-U_2)\end{aligned}\] 즉, 우리의 목표인 ${z_1, z_2}$는 다음과 같이 구해진다. \[z_1 = \sqrt{-2\ln(U_1)} \cos (2\pi U_2) \\ z_2 = \sqrt{-2\ln(U_1)} \sin (2\pi U_2)\] uniform 만 잘 뽑혔다면 두 방법은 완전히 같은 결과를 보여준다. 이번 경우는 운이 좋은 경우였고, 그렇지 않은 경우들에 대해 일반적인 방법이 필요하다. 특별히 단변량 분포에 대해서는 rejection sampling과 importance sampling 기법이 있는데 이에 대해 알아보자. 11.1.2 거부 표집법 (Rejection Sampling) 일반적인 분포 ${p(z)}$에서 표집을 할때 사용하는 간단한 기법이다. ${p(z)}$와 개형이 비슷한 적절하게 해석적인 함수 ${q(z)}$를 잡는다. 모든 구간에서 ${kq(z) \geq p(z)}$를 만족하는 상수 ${k}$를 구한다. 작을수록 좋다! ${q(z)}$를 앞서 배운 방법들로 sampling해 ${z_0}$를 얻는다. 구간 ${[0, kq(z_0)]}$에서 uniform sampling을 해 ${u)0}$를 얻는다. 만약 ${u_0 &gt; p(z)}$면 버리고(reject) 그렇지 않다면 유지한다. 11.1.3 적응적 거부 표집법 적절한 ${q(z)}$를 찾는것이 어려울 경우를 위한 방법(야매)이다. 먼저, ${\ln(p(z))}$가 오목함수인 경우에는 단순히 line interpolation method로 해결이 가능하다. numerical analysis 첫시간에 배우는 무식한 방법이지만 빠르고 좋은 방법이다. 여기서도 볼줄이야. 당연히 수집하는 point의 수가 늘어날 수록 ${p(z)}$와 ${q(z)}$가 비슷해질 것이며 rejection rate도 줄어들 것이다. 각 point에서 미분이 정의되지 않는데 변형 알고리즘이 존재한다고 한다. 로그 오목이 아닐경우 11.2.2에서 살펴본다. aka Adaptive Rejection Metropolis Sampling 고차원 공간에서는 acception rate가 기하급수적으로 감소하기 때문에 이를 개선하기 위해 복잡도가 급격히 증가하기에 사용하기 어렵다. 11.1.4 중요도 표집법 역시 표집하기 어려운 분포가 주어졌다고 가정하고 기닷갮 ${E(z)}$를 구해보자. 이번에도 새로운 분포 ${q(z)}$를 두고 일단 샘플링을 해서 ${z^l}$들을 얻어내고 해당 ${z^l}$에서 ${\frac{p(z)}{q(z)}}$만큼의 weight를 주고 기댓값을 계산해주는 방식이다. 기본 idea는 Rejection Sampling과 다를바 없어 보인다. 논리대로 식을 전개해보자면 다음과 같다. \[E[f] = \int f(z)p(z)dz = \frac{Z_q}{Z_p}\int f(z)\frac{\tilde{p}(z)}{\tilde{q}(z)}q(z)dz \simeq \frac{Z_q}{Z_p}\frac{1}{L} \sum_{l=1}^{L} \tilde{r}_{l} f(z^{(l)})\] 마찬가지로 ${q(z)}$가 얼마나 ${p(z)}$와 비슷한 분포를 가지는지, ${p(z)}$의 자연스러운 분포 여부(적당히 생긴)에 따라 기댓값의 정확도가 상승한다. 그래프 모델에서는 여기에 마르코프한 가중치를 덮어주면 된다. 또한, 이를 발전시킨 형태로 Likelihood Weighted Sampling(가능도 가중 표집법)이 존재한다. 이는 Ancestral Sampling(조상 표집법)을 적용한 형태로 진행. 더 나아가 Self Importance Sampling(자가 중요도 표집법)도 있는데 그만 알아보자. 11.1.5 표집 중요도 재표집 Rejection Sampling을 할때 ${k}$를 구하는 과정이 있었는데, 이 과정을 생략해주는 방법이다. ${q(z)}$로부터 ${z^l}$을 추출한다. 각 ${z^l}$에 대한 가중치 ${w_l}$을 계산한다. 각각의 확률이 ${w_l}$을 가지는 이산 분포 ${z^l}$들로부터 다시 ${L}$개를 추출한다. 결국 Rejection Sampling에 Importance Sampling을 덮어준 형태인데, 기본 idea는 같은 것이라 크게 발전된 형태로 보이지는 않는다. 11.1.6 표집법과 EM 알고리즘 EM을 사용할 때, E단계에서 non-analytic한 함수를 만났을 경우 표집법을 통한 근사로 해결이 가능하다. 즉, 적분꼴로 표현된 최적화 대상을 이산 합 형태로 아래와 같이 바꿔주는 것이다. \[Q(\theta, \theta^{old}) = \int p(\textbf{Z} | \textbf{X} , \theta^{old}) \ln p(\textbf{Z},\textbf{X} | \theta) \, d\textbf{Z}\] \[to\] \[Q(\theta, \theta^{old}) \approx \frac{1}{L} \sum \ln p(\textbf{Z}^{(l)}, \textbf{X} \bar \theta )\] 이 과정을 Monte Carlo EM algorithm이라고 부른다. 참고) IP 알고리즘 ${p(\textbf{Z} \mid \textbf{X})}$에서 표집이 어려운 상황에서 Maximum Likelihood 대신 Bayesian하게 접근해서 해결하는 방법. I Step \[p(\textbf{Z} | \textbf{X}) = \int p(\textbf{Z} | \textbf{X} , \theta) p(\theta | \textbf{X}) \, d\theta\] 순서대로 ${\textbf{X}}$로부터 ${\theta ^{(l)}}$들을 추출하고 이들의 결합분포를 통해 ${p(\textbf{Z} \mid \textbf{X} , \theta)}$에서 ${\textbf{X}}$를 추출한다. P Step \[p(\theta | \textbf{X}) = \int p(\theta | \textbf{Z} , \textbf{X}) p(\textbf{Z} | \textbf{X}) \, d\textbf{Z}\] 구한 ${\textbf{Z}}$들을 통해 ${\theta}$에 대한 사후분포식을 수정해준다. 11.2 마르코프 연쇄 몬테 카를로 MCMC는 Rejection Sampling이나 Importance Sampling과 비슷한 방식으로 진행되지만, 무조건 Reject되기 보다는 회생의 기회를 준다는점에서 차이가 있다. 물론 여기서 얻은 표본들도 독립이 아니지만, 충분히 큰 M에 대해 kM번째 샘플만 취해준다면 어느정도 독립성을 보장해줄 수 있다. 먼저, 가장 대표적인 방식인 Metropolis Algorithm에 대해 알아보자. 목표 분포 ${p(z)}$에 대해 Rejection Sampling때와 마찬가지로 적절한 ${q(z)}$를 잡아준다. (Metropolis 알고리즘에서는 대칭(${q(z_A \mid z_B) = q(z_B \mid z_A)}$)임을 가정한다.) 초깃값 ${z^{(1)}}$을 시작으로 ${q(z)}$ 에서 하나의 표본 ${z^*}$을 추출한다. Accept or Reject ${p(z^*) &gt; p(z^{(1)})}$일경우 accept 그렇지 않을 경우, $\frac{p(z^*)}{p(z^{(1)})}$의 확률로 accept한다. 이를 다시표현하면, ${min \left(1, \frac{p(z^*)}{p(z^{(1)})}\right)}$의 확률로 Accept한다. 이에 대한 증명은 적혀있지 않은데, 11.2.2에서 일반화된 증명으로 살펴본다. 11.2.1 마르코프 연쇄 다음 상태가 현재 상태에만 의존할 경우 마르코프하다고 한다. \[{ p(z^{(m+1)} \mid z^{(1)}, \ldots z^{(m)}) = p(z^{(m+1)}\mid z^{(m)} )}\] 각 상태에서 ${T_{m}^{ij} = p(z^{(m+1)} = v_j \mid z^{(m)} = v_i) }$을 Transition Matrix라고 부르며 이 matrix가 ${m}$에 무관할 경우 Homogenous한 chain이라고 부른다. 최종적으로 ergodic 개념이 중요한데, 아래 세 조건은 동치이다. Ergodic Positive Recurrent, Aperiodic Finite, Irreducible, Aperiodic 즉, ergodic하면, 충분한 시간이 흐른 뒤 분포와 초기 분포가 동일한 분포가 된다. 또한, 헷갈리는 개념으로 stationary가 있는데, transition matrix ${T}$에 대해 ${\pi = \pi T}$를 만족할 때, 즉, 분포가 일정할 때 stationary(invariant)하다고 한다. \[p(z^{(m+1)}) = \sum p(z^{(m+1)} \mid z^{(m)}) p(z^{(m)})\] 일반적으로 ergodic하면 stationary한 ${\pi}$가 존재하며 stationary하다. 자세한 내용이 궁금하다면 Stochastic Process를 공부해보자. 재밌는 책이다. 11.2.2 메트로폴리스 헤이스팅스 알고리즘 앞선 Metropolis Algorithm은 대칭 ${q(z)}$만을 사용했는데, 한계가 있으므로 이번에는 좀더 일반적인 상황을 위한 해법을 살펴보자. 유일한 차이는 Accept or Reject할 때이다. 다음과 같이 acceptance rate를 정규화해주면 된다. \[rate_{z, z'} = min\left(1, \frac{\frac{p(z^*)}{q(z^* \mid z^m)}}{\frac{p(z^m)}{q(z^m \mid z^*)}} \right)\] 이제 이렇게 얻은 표본집합이 ${p(z)}$에 수렴한다는 것을 증명해야한다. 책에는 좀 대충나와있는 것 같다. 일단, 각 표본들은 위의 rate를 따르는 MC라고 볼 수 있다. 나아가, acceptance rate를 통해 postivie recurrent함을 확인할 수 있고, 당연하게 Aperiodic하므로, Ergodic한 MC임을 쉽게 볼 수 있다. 따라서, unique한 stationary matrix ${\pi}$를 가짐을 알 수 있다. 이제, ${\pi}$가 ${p(x)}$에 수렴함을 보여야한다. 다음 식을 살펴보자. \[\begin{aligned}p(z)q(z'\mid z) * rate_{z, z'} &amp;= min\left(p(z)q(z'\mid z), p(z')q(z\mid z')\right) \\ &amp;= min\left(p(z')q(z\mid z'), p(z)q(z'\mid z)\right) \\ &amp;= p(z')q(z\mid z') * rate_{z', z}\end{aligned}\] 따라서, ${q(z)}$와 rate의 결합은 이 MC의 transition probability라고 볼 수 있으며, 결국 같은 ${\pi}$에 수렴함을 알 수 있다. 마찬가지로, 어떤 ${q(z)}$를 사용하는지에 따라 알고리즘의 성능에 현저한 영향을 줄 수 있다. 일반적으로 gaussian을 사용한다. 여기서 분산을 어떻게 두냐에 따른 trade-off가 있는데, 분산이 작을 경우 acceptance는 증가하지만, 전체를 탐색하기 오래걸리고, 분산이 클 경우 acceptance 자체가 낮아지게 된다. 따라서 그림 11.10과 같이 방향에 따라 분산의 차이가 클 경우 알고리즘이 느리게 수렴하게 된다. 11.3 깁스 표집법 알고보니 Gibbs Free Energy를 정의한 그 Gibbs이다. 이쪽업계 코시 같은 사람인듯; 아무튼 Gibbs Sampling은 MCMC의 한 종류이다. 방법은 간단한데, ${m}$개의 확률변수들이 결합된 결합확률분포가 있을 때, ${m - 1}$개의 변수를 고정시키고 남은 한개를 조건부 확률분포로 부터 표집하고, 다른 ${m-1}$개를 고정시키고 남은 한개를 표집하고 … 을 반복해서 얻어나가는 방법이다. 너무 rough하게 적은것 같으니 좀더 정확하게 설명하면 ${T}$회 반복하는 경우 아래와 같다. initialize ${z_i : i = 1, \ldots M}$ for ${i}$ in range (${1}$, ${T}$) ${p(z_1 \mid z_2^{i},z_3^{i},\ldots,z_M^{i})}$에서 ${z_1^{i+1}}$을 표집한다. ${p(z_2 \mid z_1^{i+1},z_3^{i},\ldots,z_M^{i})}$에서 ${z_2^{i+1}}$을 표집한다. ${\vdots}$ ${p(z_M \mid z_1^{i+1},z_2^{i+1},\ldots,z_{M-1}^{i+1})}$에서 ${z_M^{i+1}}$을 표집한다. 여기서는 변수들에게 order를 부여하고 진행했는데, random한 order로 진행할 수도 있다. 물론 결과적으로는 같다. 이제, 이 기법이 올바른 샘플링임을 증명해보자. MH 알고리즘의 일종이라고 언급했으니 이게 왜 MH 알고리즘인지 증명해보면 된다. 좀 rough하게 살펴보자. 일단, 이 분포 또한 MH에서 사용한 분포와 같이 stationary함을 보여야한다. 이는 꽤나 자명한데, 각 표집에서 적절한 조건부 분포를 사용하기 때문이다. 즉, 적할한 ${\pi}$가 존재한다. \[\begin{aligned} rate &amp;= min\left( 1, \frac{p(z^*)q(z|z^*)}{p(z)q(z^*|z) } \right)\\ &amp;= min\left(1, \frac{p(z^*_k|z^*_{-k})p(z^*_{-k})p(z_k | z^*_{-k})}{p(z_k|z_{-k})p(z_{-k})p(z^*_{k}|z_{-k})} \right) \\ &amp;= min(1,1) = 1\end{aligned}\] 즉, MH와 같은 rate를 가지고 있기 때문에, MH의 일종으로 볼 수 있다. 11.4 조각 표집법 계속해서 보이는 알고리즘들의 단점은 ${p(z)}$와 ${q(z)}$의 차이에서 비롯되었다. 분산이 작으면 탐색이 느리고, 크면 acceptance가 낮아진다. 조각표집법(slice sampling)은 새로운 방식으로 해결하고자 했다. 아이디어는 간단한데, 아래와 같이 y축에서 쳐다보는 변수 ${u}$를 추가해서 해결하는 것이다. \[\hat p(z,u) = \begin{cases} 1/Z_p &amp; 0 \leq u \leq \hat p(z) \\ 0 &amp; else \end{cases}\] 그리고, 깁스 샘플링 같은 방법을 통해 ${\hat p(z,u)}$에서 표본을 얻고, ${u}$를 무시해주어 원 분포를 얻는 방식이다. 근데 이건 그냥 원래 있던 inverse cdf하는 방법보다도 원시적인 방법이 아닌가 싶다. 11.5 하이브리드 몬테 카를로 알고리즘 11.6 분할 함수 추정]]></summary></entry><entry><title type="html">유산소 인터벌 도입</title><link href="http://localhost:4000/%EC%9A%B4%EB%8F%99/%EC%9D%B8%ED%84%B0%EB%B2%8C-%ED%9E%98%EB%93%9C%EB%84%A4/" rel="alternate" type="text/html" title="유산소 인터벌 도입" /><published>2022-07-14T00:00:00+09:00</published><updated>2022-07-14T00:00:00+09:00</updated><id>http://localhost:4000/%EC%9A%B4%EB%8F%99/%EC%9D%B8%ED%84%B0%EB%B2%8C-%ED%9E%98%EB%93%9C%EB%84%A4</id><content type="html" xml:base="http://localhost:4000/%EC%9A%B4%EB%8F%99/%EC%9D%B8%ED%84%B0%EB%B2%8C-%ED%9E%98%EB%93%9C%EB%84%A4/"><![CDATA[<p>개인 유산소 운동은 페이스 유지하면서 달리는 일반적인 조깅만 해왔었는데, 최근에 인터벌을 도입하고 오늘 2번째로 뛰어보았다.</p>

<p>효과가 있는지는 풋살할 때 확인할 수 있겠지만, 일단 너무 힘듬.. 원래 인터벌하시는 분들 보면 전속력-천천히를 한세트로 10번 정도 조지시던데 나는 5세트정도에 뻗어버림; 전속력이 진짜 전속력이 아닌건지 천천히를 진짜 완전 천천히로 해야하는지 뭔가 잘못된거 같기도 한데, 좀더 알아봐야겠다.</p>

<p>지금은 이렇게 달리고 있다.</p>
<ul>
  <li>전속력 : 3분 초반 페이스로 30초</li>
  <li>천천히 : 6분 초반 페이스로 90초</li>
</ul>

<p>일단 지금까지 느낀 조깅과 인터벌 차이.</p>

<p>조깅</p>
<ul>
  <li>낮은 젖산농도 구간에서 오랜시간 지방을 태울 수 있어 체지방 태우는데 더 좋은 느낌</li>
  <li>생각 정리에 도움이 되고 몸이 맑아지는 기분</li>
</ul>

<p>인터벌</p>
<ul>
  <li>젖산농도 끝에서 조져서 지방이고 뭐고 그냥 대둔근 타겟으로 조져지는 느낌</li>
  <li>생각을 할 수가 없고 뇌의 미세혈관이 막혀버리는 기분</li>
  <li>가끔 자전거를 추월할 때 기분이 좋다. 물론 곧 따라잡히지만.</li>
</ul>]]></content><author><name>Jaehyun Kim</name></author><category term="운동" /><category term="유산소" /><summary type="html"><![CDATA[개인 유산소 운동은 페이스 유지하면서 달리는 일반적인 조깅만 해왔었는데, 최근에 인터벌을 도입하고 오늘 2번째로 뛰어보았다. 효과가 있는지는 풋살할 때 확인할 수 있겠지만, 일단 너무 힘듬.. 원래 인터벌하시는 분들 보면 전속력-천천히를 한세트로 10번 정도 조지시던데 나는 5세트정도에 뻗어버림; 전속력이 진짜 전속력이 아닌건지 천천히를 진짜 완전 천천히로 해야하는지 뭔가 잘못된거 같기도 한데, 좀더 알아봐야겠다. 지금은 이렇게 달리고 있다. 전속력 : 3분 초반 페이스로 30초 천천히 : 6분 초반 페이스로 90초 일단 지금까지 느낀 조깅과 인터벌 차이. 조깅 낮은 젖산농도 구간에서 오랜시간 지방을 태울 수 있어 체지방 태우는데 더 좋은 느낌 생각 정리에 도움이 되고 몸이 맑아지는 기분 인터벌 젖산농도 끝에서 조져서 지방이고 뭐고 그냥 대둔근 타겟으로 조져지는 느낌 생각을 할 수가 없고 뇌의 미세혈관이 막혀버리는 기분 가끔 자전거를 추월할 때 기분이 좋다. 물론 곧 따라잡히지만.]]></summary></entry><entry><title type="html">인간실격</title><link href="http://localhost:4000/book/%EC%9D%B8%EA%B0%84%EC%8B%A4%EA%B2%A9/" rel="alternate" type="text/html" title="인간실격" /><published>2022-07-10T00:00:00+09:00</published><updated>2022-07-10T00:00:00+09:00</updated><id>http://localhost:4000/book/%EC%9D%B8%EA%B0%84%EC%8B%A4%EA%B2%A9</id><content type="html" xml:base="http://localhost:4000/book/%EC%9D%B8%EA%B0%84%EC%8B%A4%EA%B2%A9/"><![CDATA[<h2 id="후기">후기</h2>

<p>전개가 어지러워서 정신을 차리고 읽느라 애를 먹었다. 호밀밭의 파수꾼을 다른 버전으로 읽는 느낌을 받았고 일본 소설만의 표현하기 어려운 그 특유의 분위기가 조금은 불쾌하기도 했다.</p>

<p>주인공 요조는 그야말로 염세주의에 제대로 빠진 사람으로 보였는데, 낙관주의에 가까운 나로서는 그의 가정환경과 일련의 행동들이 그저 안타깝게 보일 뿐이었다.</p>

<p>요조의 허무주의는 분명 패전으로 무너진 일본인들을 대변해주는 좋은 인물이었을 것 같다. 더구나 자살을 보는 사회적인 시각이 지금과 같지 않고 본인의 존엄성을 지키기 위한 마지막 자주적 선택 정도였던 일본 문화를 비추어 보았을 때 당시 이 책이 일본인들 사이에서 선풍적인 인기를 끌었음은 짐작이 간다.</p>

<p>그런데 아직까지도 사랑받는 이유는 사실 잘 모르겠다. 한국에 요조에 공감할 정도로 염세주의와 허무주의에 빠진 사람들이 이렇게 많은가? 당시 일본의 상황에 이입해서 읽고 있는 것인가? 잘은 이해되지 않는다만 2030에서 판매량이 많았던 것을 통해 최대한 유추해보면 전자에 가깝지 않을까 싶다.</p>

<p>조금 더 날 것의 표현을 하자면 그가 잘생긴 얼굴을 바탕으로 주변 헬스인의 권유로 운동에 취미를 붙였다면 이 정도로 염세주의에 과몰입하지는 않았을 것으로 생각한다.</p>

<h4 id="추천하는-독자">추천하는 독자</h4>

<p>파괴된 삶의 날선 묘사가 궁금한 사람</p>

<h4 id="추천하지-않는-독자">추천하지 않는 독자</h4>

<p>허무주의 부류에 빠진 사람</p>

<p>비도덕적인 요소들에 불쾌감을 느끼는 독자</p>

<h4 id="한-문장">한 문장</h4>

<blockquote>
  <p>부끄럼 많은 생애를 보내왔습니다. 저는 인간의 삶이라는 것을 도무지 이해할 수 없습니다.</p>
</blockquote>

<p>그래 .. 알고있다 …</p>]]></content><author><name>Jaehyun Kim</name></author><category term="book" /><category term="novel" /><summary type="html"><![CDATA[후기]]></summary></entry><entry><title type="html">2022 상반기 회고</title><link href="http://localhost:4000/misc/2022-%EC%83%81%EB%B0%98%EA%B8%B0-%ED%9A%8C%EA%B3%A0/" rel="alternate" type="text/html" title="2022 상반기 회고" /><published>2022-07-07T00:00:00+09:00</published><updated>2022-07-07T00:00:00+09:00</updated><id>http://localhost:4000/misc/2022-%EC%83%81%EB%B0%98%EA%B8%B0-%ED%9A%8C%EA%B3%A0</id><content type="html" xml:base="http://localhost:4000/misc/2022-%EC%83%81%EB%B0%98%EA%B8%B0-%ED%9A%8C%EA%B3%A0/"><![CDATA[<h2 id="상반기-목표">상반기 목표</h2>

<p>대략 3월쯤 세운 상반기 <strong>기존 목표</strong>는 아래와 같다.</p>

<ul>
  <li>ML 스터디 성실히 진행</li>
  <li>학점 4 이상</li>
  <li>코포 시작, 퍼플 레이팅 만들기 (올해 내로)</li>
  <li>건강 관리 4점 이상</li>
  <li>돈 관리</li>
</ul>

<h2 id="상반기-결산">상반기 결산</h2>

<p>결과적으로 6월까지 해온 것들은 다음과 같다.</p>

<ul>
  <li>ML 스터디 진행중
    <ul>
      <li><a href="http://www.yes24.com/Product/Goods/88440860">단단한 머신러닝</a> 완주</li>
      <li><a href="https://puffy-stick-fa1.notion.site/2022-ca5f43fc259d446d81f376256d18b99b">우아한 스터디</a>에서 <a href="http://www.yes24.com/Product/Goods/64189352">PRML</a> 진행중</li>
    </ul>
  </li>
  <li>학점 4.1</li>
  <li>코포 블루 달성 (초반에 달성 후 중단 중)</li>
  <li>건강 관리 3점
    <ul>
      <li>초반 4점(식단, 헬스) -&gt; 후반 2점(간헐적 러닝)</li>
      <li>영양제는 잘 챙겨먹음(종비, 칼맥, 유산균, 오메가3)</li>
    </ul>
  </li>
  <li>
    <p>과외 2개, 튜터링 1개로 유지</p>
  </li>
  <li>네이버 파이낸셜 BE직군 합격</li>
  <li>스타트업 BE 포지션 제안</li>
  <li><del>롤 골드, 롤체 다이아(초반에 달성)</del></li>
</ul>

<h2 id="요약">요약</h2>

<p>학기 초에는 알고리즘만 주구장창 공부하면서 좀 놀았고,</p>

<p>학기 중에는 ML 공부에 집중하고,</p>

<p>학기 말에는 기말고사와 회사 면접으로 정신없이 보냈다.</p>

<h2 id="회고">회고</h2>

<p>총점을 주자면 4점 정도 주고 싶다.</p>

<p>학기와 병행하면서 CS지식 처음부터 쌓아서 네이버에 최종 합격한 점에서 높은 점수를 주고 싶고, 아쉬운 점이라면 생활패턴을 지키지 못해서 건강 관리를 놓친 거? 아직은 그래도 젊어서 별 문제는 없었다만 더 나이들기 전에 확립해야한다는 생각을 종종한다,,</p>

<p>그리고 독립 생활 유지 또한 잘 해오고 있고 이것저것 찾아서 지원금 받고 스터디하고 야무지게 살고 있는듯.</p>

<h2 id="2022-하반기-계획">2022 하반기 계획</h2>

<p>일단 교수님과 면담을 통해 석사 휴학을 하기로 했고, 당분간 회사를 다닐 것 같다. 어떤 회사에서 어떤 직군으로 다닐지 한 달 동안 고민을 했는데, 석사 졸업하면 평생 BE 직군으로 일할 기회는 굳이 없을 것 같기도 하고 마침 직무도 흥미가 끌리는 쪽이라서 돈도 모을겸 네이버 다니는게 좋은 판단일 것 같다.</p>

<p>뭐 또 바뀌겠지만 지금 목표도 세워보았다.</p>

<ul>
  <li><strong>JAVA, Spring 공부</strong>
    <ul>
      <li>사실 java/spring 경험자가 지원자격이였는데 전혀 모르는 내가 어떻게 붙은지 모르겠다;; 심지어 자기소개서 서류도 잘못냈는데 ,,</li>
      <li>어쨌든 다니기로 했으니 빠르게 입사 전까지 공부해가야 할듯.</li>
      <li>물론 회사 가서도 계속 배워야지 ,,</li>
      <li>+) 클린코드 읽기</li>
    </ul>
  </li>
  <li>코포 퍼플 달성
    <ul>
      <li>왠지 조금 시간 투자하면 금방 갈 것 같은 느낌</li>
    </ul>
  </li>
  <li>ML 스터디 지속</li>
  <li>건강 관리
    <ul>
      <li>일단 이번엔 집앞 헬스장 등록하고.. 집 앞에 테니스장이 있길래 테니스를 배워볼까 생각 중이다. 물론 러닝은 꾸준히</li>
      <li>피부과 매달 방문</li>
    </ul>
  </li>
  <li>돈 관리
    <ul>
      <li>이제 월급을 받으니 ,, 좀더 구체적으로 ,,</li>
    </ul>
  </li>
</ul>]]></content><author><name>Jaehyun Kim</name></author><category term="misc" /><category term="etc" /><summary type="html"><![CDATA[상반기 목표]]></summary></entry><entry><title type="html">Github 블로그 개장</title><link href="http://localhost:4000/misc/hello-blog/" rel="alternate" type="text/html" title="Github 블로그 개장" /><published>2022-07-05T00:00:00+09:00</published><updated>2022-07-05T00:00:00+09:00</updated><id>http://localhost:4000/misc/hello-blog</id><content type="html" xml:base="http://localhost:4000/misc/hello-blog/"><![CDATA[<h2 id="드디어-개장">드디어 개장!</h2>

<p>중학교 ~ 대학교 1학년 때까지 네이버 블로그를 사용하다 네이버의 정책이 맘에 안들어 티스토리로 도망쳤다가, 결국 그놈이 그놈이라 근본 github으로 넘어왔다!</p>

<h2 id="과정">과정</h2>

<p>우선 <strong>TOC</strong>가 꼭 있는 테마를 추려내고, 디자인이 맘에 드는 것들을 골랐는데,
처음에는 <a href="https://jeffreytse.github.io/jekyll-theme-yat/">yat</a>과 <a href="https://just-the-docs.github.io/just-the-docs/">just-the-docs</a> 테마가 선정되었다!</p>

<p>근데 docs가 좀 부실하고 자잘한 이슈들이 있어, 결국 docs 잘돼어 있고 많이들 쓰는 <a href="https://github.com/mmistakes/minimal-mistakes">minimal-mistakes</a>로 정착하게 되었다. 역시 많이 쓰는덴 이유가 있어…</p>

<h2 id="1일-후기">1일 후기</h2>

<p>일단 css를 직접 만질 수 있어서 너무 편하다~~</p>

<p>말그대로 jekyll을 사용한다는 것만 빼면 첨부터 빌드하는건데 역시 내 손으로 직접 쌓아야 마음이 편한듯 ,,</p>

<p>그리고 무엇보다 post도 markdown으로 슥슥 써내릴 수 있어서 부담감이 덜하다 !</p>

<p>학부 때 맨날 하던게 latex로 과제 짜고 typora로 필기하는거였는데 이게 블로그에서도 가능하다니 !
(더 이상 마우스로 손이 가지 않아도 되는거야)</p>

<h2 id="운영-방향">운영 방향</h2>

<p>네이버 블로그할 때는 누군가에게 설명하기 위한 글을 썼었는데, 아무래도 그런식의 블로깅은 부담감이 있어 잘 안쓰게 된다,,</p>

<p>따라서, 이번 블로그는 애초에 검색 노출부터 따로 설정하지 않고 공부한/했던 내용들을 아카이빙하는 방향으로 했다 !</p>

<p>짧은 가방끈이지만 나름 다양한 공학 영역의 공부를 했기에,, 적을 수 있는 내용이 많을 것 같다 !</p>]]></content><author><name>Jaehyun Kim</name></author><category term="misc" /><category term="etc" /><summary type="html"><![CDATA[드디어 개장!]]></summary></entry></feed>